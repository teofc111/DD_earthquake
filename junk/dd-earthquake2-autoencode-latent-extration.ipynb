{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":8794013,"datasetId":5242394,"databundleVersionId":8950922}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"177885c36077483aa9a6c289e78cdc0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e01ea060321941daae6a4920f6b14300","placeholder":"​","style":"IPY_MODEL_f9bbe433a83c42829870f3835095e2d2","value":"Best trial: 67. Best value: 0.443385: 100%"}},"189ea666398a4ebba4bf374ceba7e79f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_967457e466314976b434375cbdc00e07","placeholder":"​","style":"IPY_MODEL_8079bcb47bfd4e66bb5e7ac8ca182c90","value":" 100/100 [1:10:13&lt;00:00, 44.86s/it]"}},"289f8b6cf6a848879970cce0ff201c91":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_177885c36077483aa9a6c289e78cdc0e","IPY_MODEL_484595e3c7254a11988633fcdadba889","IPY_MODEL_189ea666398a4ebba4bf374ceba7e79f"],"layout":"IPY_MODEL_2d6cd261b56c4817a7fbf0a2888fca56"}},"2d6cd261b56c4817a7fbf0a2888fca56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"484595e3c7254a11988633fcdadba889":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f543358ccb404ef5bbab84448c280f6d","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c683dea3a9940d39e8af70808de03af","value":100}},"5c683dea3a9940d39e8af70808de03af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8079bcb47bfd4e66bb5e7ac8ca182c90":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"967457e466314976b434375cbdc00e07":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e01ea060321941daae6a4920f6b14300":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f543358ccb404ef5bbab84448c280f6d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9bbe433a83c42829870f3835095e2d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install catboost optuna","metadata":{"id":"d1mzITNGr_jV","outputId":"aa4b199e-6f96-4139-c67d-a41da99408be","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n\n# # Mount Google Drive\n# drive.mount('/content/drive')","metadata":{"id":"5INR792OsFkX","outputId":"b21df584-a463-420e-ed87-21d9b913c0dd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import catboost as cb\n\n# import shap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, f1_score, roc_auc_score, accuracy_score, silhouette_score, pairwise_distances\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.manifold import MDS\nfrom sklearn.cluster import KMeans\n\n# import optuna\n\nfrom scipy.spatial.distance import pdist, squareform\n\nimport pickle\n\nfrom scipy.sparse import csr_matrix, coo_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport xgboost as xgb\n","metadata":{"id":"BOu-dHcFrFTe","execution":{"iopub.status.busy":"2024-07-03T08:59:43.246062Z","iopub.execute_input":"2024-07-03T08:59:43.246662Z","iopub.status.idle":"2024-07-03T08:59:49.273206Z","shell.execute_reply.started":"2024-07-03T08:59:43.246633Z","shell.execute_reply":"2024-07-03T08:59:49.272178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resources:  https://towardsdatascience.com/catboost-regression-in-6-minutes-3487f3e5b329","metadata":{"id":"ET9k0zMerFTh"}},{"cell_type":"markdown","source":"### Load and Clean Data","metadata":{"id":"F50O2DI7rFTj"}},{"cell_type":"markdown","source":"#### General","metadata":{}},{"cell_type":"code","source":"# datapath = './data/' # local\n# datapath = r'/content/drive/MyDrive/Colab Notebooks/DD_earthquake_data/' # colab\ndatapath = r'/kaggle/input/' # kaggle\n\ndf_train_features_raw = pd.read_csv(datapath+'train_values.csv').set_index('building_id')\ndf_train_labels_raw = pd.read_csv(datapath+'train_labels.csv').set_index('building_id')\ndf_pred_features_raw = pd.read_csv(datapath+'test_values.csv').set_index('building_id')","metadata":{"id":"J2miD3pFrFTj","execution":{"iopub.status.busy":"2024-07-03T09:00:13.346783Z","iopub.execute_input":"2024-07-03T09:00:13.347462Z","iopub.status.idle":"2024-07-03T09:00:14.925601Z","shell.execute_reply.started":"2024-07-03T09:00:13.347433Z","shell.execute_reply":"2024-07-03T09:00:14.924801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- No missing data to fill","metadata":{"id":"SGy9KKAprFTk"}},{"cell_type":"code","source":"def preprocess(df_features):\n    # Convert numeric features to float\n    df_features = df_features.astype({'count_floors_pre_eq':'float',\n                                      'age':'float',\n                                      'area_percentage':'float',\n                                      'height_percentage':'float',\n                                      'count_families':'float'})\n    return df_features","metadata":{"id":"yv3q5-CbrFTk","execution":{"iopub.status.busy":"2024-07-03T09:00:17.073850Z","iopub.execute_input":"2024-07-03T09:00:17.074669Z","iopub.status.idle":"2024-07-03T09:00:17.079197Z","shell.execute_reply.started":"2024-07-03T09:00:17.074639Z","shell.execute_reply":"2024-07-03T09:00:17.078171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature groups\ncategorical_features = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id', 'land_surface_condition', 'foundation_type', 'roof_type', 'ground_floor_type', 'other_floor_type', 'position', 'plan_configuration', 'has_superstructure_adobe_mud', 'has_superstructure_mud_mortar_stone', 'has_superstructure_stone_flag',\n                        'has_superstructure_cement_mortar_stone', 'has_superstructure_mud_mortar_brick', 'has_superstructure_cement_mortar_brick', 'has_superstructure_timber', 'has_superstructure_bamboo', 'has_superstructure_rc_non_engineered', 'has_superstructure_rc_engineered', 'has_superstructure_other', \n                        'legal_ownership_status', 'has_secondary_use', 'has_secondary_use_agriculture', 'has_secondary_use_hotel', 'has_secondary_use_rental', 'has_secondary_use_institution', 'has_secondary_use_school', 'has_secondary_use_industry', 'has_secondary_use_health_post', 'has_secondary_use_gov_office', 'has_secondary_use_use_police', 'has_secondary_use_other']\ngeo_features = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\nnumerical_features = ['count_floors_pre_eq', 'age', 'area_percentage', 'height_percentage', 'count_families']\n# non_geo_features_w_label = categorical_features + numerical_features + ['damage_grade']\n# for geo_feat in geo_features:\n#     non_geo_features_w_label.remove(geo_feat)","metadata":{"id":"BrgrVl4XrFTk","execution":{"iopub.status.busy":"2024-07-03T09:00:17.276969Z","iopub.execute_input":"2024-07-03T09:00:17.277813Z","iopub.status.idle":"2024-07-03T09:00:17.284178Z","shell.execute_reply.started":"2024-07-03T09:00:17.277781Z","shell.execute_reply":"2024-07-03T09:00:17.283169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AutoEncode Geography (Pytorch)","metadata":{}},{"cell_type":"markdown","source":"#### Impute damage_grade of test features","metadata":{}},{"cell_type":"code","source":"def impute_damage_grade(df_train_X: pd.DataFrame,df_val_X: pd.DataFrame,\n                        df_test_X: pd.DataFrame,df_pred_X: pd.DataFrame,\n                        df_train_y: pd.DataFrame) -> pd.DataFrame:\n    '''\n    Impute damage grade based on location (i.e. the three geo features). This function\n    \n    The original dataset\n    should first be split into train, val and test sets. The imputation will be done using all\n    features from all datasets, but only labels from the train dataset to prevent date leakage.\n    df_train_X: Post splitting X dataframe for training\n    df_val_X: Post splitting X dataframe for validation set\n    df_test_X: Post splt\n\n    '''\n    # Preprocess to convert numerical features to float\n    df_train_X = preprocess(df_train_X.copy())\n    df_val_X = preprocess(df_val_X.copy())\n    df_test_X = preprocess(df_test_X.copy())\n    df_pred_X = preprocess(df_pred_X.copy())\n\n    df_all = pd.concat([df_train_X,df_train_y],axis=1)\n    df_all = pd.concat([df_all,df_val_X,df_test_X,df_pred_X],axis=0)\n\n    df_all['geo_combined3'] = df_all.apply(lambda row: str(row['geo_level_1_id']) + '_' + str(row['geo_level_2_id']) + '_' + str(row['geo_level_3_id']), axis=1)\n    df_all['geo_combined2'] = df_all.apply(lambda row: str(row['geo_level_1_id']) + '_' + str(row['geo_level_2_id']), axis=1)\n\n    # Fillna for damage_grade with means for geo level 3\n    damage_means3 = df_all.groupby('geo_combined3')['damage_grade'].mean()\n    df_all['damage_grade'] = df_all.apply(lambda row: damage_means3[row['geo_combined3']] if pd.isna(row['damage_grade']) else row['damage_grade'], axis=1)\n\n    # Fillna for damage_grade with means for lowest geo level 2\n    damage_means2 = df_all.groupby('geo_combined2')['damage_grade'].mean()\n    df_all['damage_grade'] = df_all.apply(lambda row: damage_means2[row['geo_combined2']] if pd.isna(row['damage_grade']) else row['damage_grade'], axis=1)\n\n    # Fillna for damage_grade with means for lowest geo level 2\n    damage_means1 = df_all.groupby('geo_level_1_id')['damage_grade'].mean()\n    df_all['damage_grade'] = df_all.apply(lambda row: damage_means1[row['geo_level_1_id']] if pd.isna(row['damage_grade']) else row['damage_grade'], axis=1)\n\n    # Drop unwanted columns\n    df_all.drop(columns=['geo_combined2','geo_combined3'],inplace=True)\n    df_all = pd.get_dummies(df_all,columns=[col for col in categorical_features if not (col in geo_features)],drop_first=True)\n\n    # Get feature names\n    non_geo_features_OHE = [col for col in df_all.columns if not (col in geo_features)]\n\n\n    return df_all, non_geo_features_OHE","metadata":{"execution":{"iopub.status.busy":"2024-07-03T09:00:18.112499Z","iopub.execute_input":"2024-07-03T09:00:18.113586Z","iopub.status.idle":"2024-07-03T09:00:18.126832Z","shell.execute_reply.started":"2024-07-03T09:00:18.113547Z","shell.execute_reply":"2024-07-03T09:00:18.125937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train, val, and test sets\nX_train_or, df_test_X, y_train_or, df_test_y = train_test_split(df_train_features_raw, df_train_labels_raw, test_size = 0.15, random_state=42,stratify=df_train_labels_raw)\ndf_train_X, df_val_X, df_train_y, df_val_y = train_test_split(X_train_or, y_train_or, test_size = 0.15, random_state=42,stratify=y_train_or)\ndf_pred_X = df_pred_features_raw\n\n# Get imputed damage grade to prep for geo-encoding\ndf_all, non_geo_features_OHE = impute_damage_grade(df_train_X, df_val_X, df_test_X,df_pred_X, df_train_y)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T09:00:18.574799Z","iopub.execute_input":"2024-07-03T09:00:18.575412Z","iopub.status.idle":"2024-07-03T09:00:54.748176Z","shell.execute_reply.started":"2024-07-03T09:00:18.575376Z","shell.execute_reply":"2024-07-03T09:00:54.747188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Geo-features + other categorical features\nTo iterate over:\n1. architecture and weights\n2. latent size\n3. Number of epochs","metadata":{}},{"cell_type":"code","source":"# geo1_n_geo2_n_geo3_n_int_n_lat_n_epo_n\n\n# geo_n_lat_n = [(16,32,64,128,64),(16,32,64,128,32),(16,32,64,128,16),(16,32,64,128,8),(16,32,64,128,4)]\n# geo_n_lat_n += [(8,16,32,64,32),(8,16,32,64,16),(8,16,32,64,8),(8,16,32,64,4)]\n# geo_n_lat_n += [(4,8,16,64,32),(4,8,16,64,16),(4,8,16,64,8),(4,8,16,64,4)]\n# epo_n = [1,2,4,8,10,12,16,20,25,30,40]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T09:00:54.749804Z","iopub.execute_input":"2024-07-03T09:00:54.750086Z","iopub.status.idle":"2024-07-03T09:00:54.754270Z","shell.execute_reply.started":"2024-07-03T09:00:54.750063Z","shell.execute_reply":"2024-07-03T09:00:54.753315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the autoencoder\nclass CategoricalAutoencoder(nn.Module):\n    def __init__(self, geo1_size, geo2_size, geo3_size, inter_size, latent_size):\n        super(CategoricalAutoencoder, self).__init__()\n        self.geo1_size = geo1_size\n        self.geo2_size = geo2_size\n        self.geo3_size = geo3_size\n\n        # Embedding layers for each categorical input\n        self.embed1 = nn.Embedding(31, geo1_size)\n        self.embed2 = nn.Embedding(1418, geo2_size)\n        self.embed3 = nn.Embedding(11861, geo3_size)\n        \n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(geo1_size + geo2_size + geo3_size + 58, inter_size),\n            nn.ReLU(),\n            nn.Linear(inter_size, latent_size)\n        )\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_size, inter_size),\n            nn.ReLU(),\n            nn.Linear(inter_size, geo1_size + geo2_size + geo3_size + 58)\n        )\n        \n        # Output layers for each categorical input\n        self.output1 = nn.Linear(geo1_size, 31)\n        self.output2 = nn.Linear(geo2_size, 1418)\n        self.output3 = nn.Linear(geo3_size, 11861)\n        \n    def forward(self, x1, x2, x3, feats):\n        # Embedding the inputs\n        x1 = self.embed1(x1)\n        x2 = self.embed2(x2)\n        x3 = self.embed3(x3)\n        \n        # Concatenate embeddings\n        x = torch.cat([x1, x2, x3, feats], dim=1)\n        \n        # Encode\n        z = self.encoder(x)\n        \n        # Decode\n        x_recon = self.decoder(z)\n        \n        # Split the reconstructed output\n        x1_recon, x2_recon, x3_recon, feats_recon = torch.split(x_recon, [self.geo1_size, self.geo2_size, self.geo3_size, 58], dim=1)\n        \n        # Get the output logits for each categorical input\n        x1_out = self.output1(x1_recon)\n        x2_out = self.output2(x2_recon)\n        x3_out = self.output3(x3_recon)\n        \n        return x1_out, x2_out, x3_out, feats_recon\n\ndef get_latent_geo(df_all:pd.DataFrame, non_geo_features_OHE:list, geo1_size:int = 16, geo2_size:int = 32,\n                   geo3_size:int = 64, inter_size:int = 128,\n                   latent_size:int = 64, epochs:int = 10):\n    '''\n    Build autoencoder network and get latent geo-encoding based on defined architecture.\n    geo1_size: Embedding dimension for first geo level\n    geo2_size: Embedding dimension for second geo level\n    geo3_size: Embedding dimension for third geo level\n    inter_size: Output (input) size of encoder's (decoder's) first (last) linear layer, interfacing with latent values\n    epochs: Number of epochs to run model\n    '''\n    # Instantiate the model\n    model = CategoricalAutoencoder(geo1_size, geo2_size, geo3_size, inter_size, latent_size).cuda()\n\n    # Define the loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Create DataLoader\n    df = df_all.copy()\n    bool_cols = df.select_dtypes(include='bool').columns\n    df[bool_cols] = df[bool_cols].astype(float)\n\n    # Encode the categorical columns\n    le_cat1 = LabelEncoder()\n    le_cat2 = LabelEncoder()\n    le_cat3 = LabelEncoder()\n\n    df['geo_level_1_id'] = le_cat1.fit_transform(df['geo_level_1_id'])\n    df['geo_level_2_id'] = le_cat2.fit_transform(df['geo_level_2_id'])\n    df['geo_level_3_id'] = le_cat3.fit_transform(df['geo_level_3_id'])\n\n    # Convert DataFrame to tensors\n    x1 = torch.tensor(df['geo_level_1_id'].values, dtype=torch.long).cuda()\n    x2 = torch.tensor(df['geo_level_2_id'].values, dtype=torch.long).cuda()\n    x3 = torch.tensor(df['geo_level_3_id'].values, dtype=torch.long).cuda()\n\n    feat_tensor = torch.tensor(df[non_geo_features_OHE].values, dtype=torch.float).cuda()\n\n    # Create DataLoader\n    dataset = TensorDataset(x1, x2, x3, feat_tensor)\n    dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n\n    # Training loop with tqdm\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}'):\n            x1_batch, x2_batch, x3_batch, feats_batch = batch\n            optimizer.zero_grad()\n            \n            # Forward pass\n            x1_out, x2_out, x3_out, feats_recon = model(x1_batch, x2_batch, x3_batch, feats_batch)\n            \n            # Compute the loss\n            loss = criterion(x1_out, x1_batch) + \\\n                criterion(x2_out, x2_batch) + \\\n                criterion(x3_out, x3_batch)\n            \n            loss_feat = nn.MSELoss()(feats_recon, feats_batch)\n\n            total_loss = loss + loss_feat\n\n            # Backward pass and optimization\n            total_loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader)}')\n\n\n    # Extract encoding for each location\n    # Pass the input through the encoder and plot the results\n    model.eval()\n    z_list = []\n\n    eval_dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n\n    with torch.no_grad():\n        for batch in tqdm(eval_dataloader, desc='Evaluating'):\n            x1_batch, x2_batch, x3_batch, feat_tensor = batch\n            \n            # Forward pass\n            z_batch = model.encoder(torch.cat([model.embed1(x1_batch),\n                                            model.embed2(x2_batch),\n                                            model.embed3(x3_batch),\n                                            feat_tensor],\n                                            dim=1))\n        \n            # Move tensors to CPU\n            z_list.append(z_batch.cpu())\n            \n            # Clear cache\n            torch.cuda.empty_cache()\n\n    # Concatenate results\n    z = torch.cat(z_list, dim=0)\n    latent_geo = pd.concat([df_all.iloc[:,:3].reset_index(drop=True),pd.DataFrame(z.cpu().numpy())],axis=1)\n    latent_geo = latent_geo.groupby(geo_features).mean().reset_index() # Get average latent values\n\n    return latent_geo, model, dataset, x1, x2, x3","metadata":{"execution":{"iopub.status.busy":"2024-07-03T09:00:54.755811Z","iopub.execute_input":"2024-07-03T09:00:54.756187Z","iopub.status.idle":"2024-07-03T09:00:54.781918Z","shell.execute_reply.started":"2024-07-03T09:00:54.756157Z","shell.execute_reply":"2024-07-03T09:00:54.780981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"geo_n_lat_n = [(16,32,64,128,64),(16,32,64,128,32),(16,32,64,128,16),(16,32,64,128,8),(16,32,64,128,4)]\ngeo_n_lat_n += [(8,16,32,64,64),(8,16,32,64,32),(8,16,32,64,16),(8,16,32,64,8),(8,16,32,64,4)]\ngeo_n_lat_n += [(4,8,16,64,64),(4,8,16,64,32),(4,8,16,64,16),(4,8,16,64,8),(4,8,16,64,4)]\nepo_n = [1,2,4,8,10,12,16,20,25,30,40]\n\nfor combo in geo_n_lat_n:\n    for epochs in epo_n:\n        geo1_size = combo[0]\n        geo2_size = combo[1]\n        geo3_size = combo[2]\n        inter_size = combo[3]\n        latent_size = combo[4]\n\n        pickle_name = f'latent_geo_geo1_{geo1_size}_geo2_{geo2_size}_geo3_{geo3_size}_inter_{inter_size}_lat_{latent_size}_epo_{epochs}.pkl'\n\n        print(f'\\n\\n\\n~~~~~~~~~~~~~~~~~Now running {pickle_name[:-4]}~~~~~~~~~~~~~~~~~')\n\n        latent_geo,_,_,_,_,_ = get_latent_geo(df_all, non_geo_features_OHE, geo1_size, geo2_size,\n                                              geo3_size, inter_size, latent_size, epochs)\n\n        with open(pickle_name,'wb') as f:\n            pickle.dump(latent_geo,f)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T09:01:00.330416Z","iopub.execute_input":"2024-07-03T09:01:00.331112Z","iopub.status.idle":"2024-07-03T09:01:24.286956Z","shell.execute_reply.started":"2024-07-03T09:01:00.331085Z","shell.execute_reply":"2024-07-03T09:01:24.285562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}